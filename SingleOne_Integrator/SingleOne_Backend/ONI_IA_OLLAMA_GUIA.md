# ü§ñ Guia de Integra√ß√£o: Oni o S√°bio + Ollama (IA/NLP Local)

## üìã **O que foi implementado:**

‚úÖ **Servi√ßo de IA Local (`OllamaService`)**  
‚úÖ **RAG (Retrieval-Augmented Generation)** - Combina busca na base de conhecimento + IA  
‚úÖ **Integra√ß√£o com TinOneService** - Usa IA quando habilitada  
‚úÖ **Fallback inteligente** - Se IA n√£o dispon√≠vel, usa resposta gen√©rica  
‚úÖ **Logs detalhados** para debug  

---

## üîß **Como Funciona:**

1. **Usu√°rio faz uma pergunta** ao Oni
2. **Sistema tenta responder com FAQ** (busca por palavras-chave)
3. **Se n√£o encontrar, tenta identificar processo** guiado
4. **Se ainda n√£o encontrar E IA estiver habilitada:**
   - üîç Busca contexto relevante na base de conhecimento (FAQ, processos, campos)
   - ü§ñ Envia contexto + pergunta para Ollama
   - ‚ú® Ollama gera resposta personalizada usando IA
5. **Se IA n√£o dispon√≠vel:** resposta gen√©rica

---

## üì• **INSTALA√á√ÉO DO OLLAMA (Windows)**

### **Passo 1: Baixar e Instalar**

#### **Op√ß√£o A: Download Direto (Recomendado)**
1. Acesse: https://ollama.com/download/windows
2. Baixe `OllamaSetup.exe`
3. Execute o instalador
4. Siga o assistente de instala√ß√£o
5. **Reinicie o terminal** ap√≥s instala√ß√£o

#### **Op√ß√£o B: Via Winget**
```powershell
winget install Ollama.Ollama
```

#### **Op√ß√£o C: Via Chocolatey**
```powershell
choco install ollama
```

### **Passo 2: Verificar Instala√ß√£o**
```powershell
ollama --version
```

Deve retornar algo como: `ollama version 0.x.x`

---

## üß† **BAIXAR MODELO DE IA**

### **Modelos Recomendados:**

#### **1. Llama 3.2 (3B) - RECOMENDADO para iniciar**
```bash
ollama pull llama3.2:3b
```
- **Tamanho:** ~2GB
- **RAM necess√°ria:** ~4GB
- **Velocidade:** Muito r√°pida
- **Qualidade:** Boa para portugu√™s

#### **2. Mistral (7B) - Mais poderoso**
```bash
ollama pull mistral:7b
```
- **Tamanho:** ~4GB
- **RAM necess√°ria:** ~8GB
- **Velocidade:** M√©dia
- **Qualidade:** Excelente

#### **3. Phi-3 Mini (3.8B) - Alternativa leve**
```bash
ollama pull phi3:mini
```
- **Tamanho:** ~2.3GB
- **RAM necess√°ria:** ~6GB
- **Velocidade:** R√°pida
- **Qualidade:** Boa

### **Verificar Modelos Instalados:**
```bash
ollama list
```

---

## ‚öôÔ∏è **CONFIGURAR NO SINGLEONE**

### **1. Habilitar IA no Sistema**

1. Fa√ßa login no SingleOne como **administrador**
2. Acesse **Configura√ß√µes ‚Üí Oni o S√°bio**
3. Ative a op√ß√£o: **"Habilitar IA/NLP (BETA)"**
4. Clique em **"Salvar Configura√ß√µes"**
5. Aguarde 2 segundos para reload autom√°tico

### **2. Verificar se Ollama est√° Rodando**

Abra um novo terminal e execute:
```bash
ollama serve
```

**OU** verifique se o servi√ßo j√° est√° rodando:
```powershell
Get-Process -Name "ollama" -ErrorAction SilentlyContinue
```

---

## üß™ **TESTAR A INTEGRA√á√ÉO**

### **Teste 1: Verificar Disponibilidade**
```bash
curl http://localhost:11434/api/tags
```
Deve retornar JSON com lista de modelos instalados.

### **Teste 2: Perguntar ao Oni**

Com IA **DESABILITADA** (comportamento antigo):
```
Pergunta: "o que posso fazer com equipamentos no sistema?"
Resposta: "Desculpe, ainda n√£o sei responder essa pergunta. Estou aprendendo! ü§ñ..."
```

Com IA **HABILITADA**:
```
Pergunta: "o que posso fazer com equipamentos no sistema?"
Resposta: "No SingleOne, voc√™ pode gerenciar equipamentos de diversas formas! üñ•Ô∏è

Aqui est√£o as principais funcionalidades:

‚Ä¢ **Cadastrar Equipamentos** - Adicione novos equipamentos ao invent√°rio com informa√ß√µes t√©cnicas e fiscais
‚Ä¢ **Movimentar Equipamentos** - Fa√ßa entregas, devolu√ß√µes e transfer√™ncias entre colaboradores
‚Ä¢ **Criar Requisi√ß√µes** - Solicite equipamentos para colaboradores conforme pol√≠tica de elegibilidade
‚Ä¢ **Consultar Relat√≥rios** - Veja equipamentos por status, garantias pr√≥ximas do vencimento, e muito mais!
‚Ä¢ **Exportar Dados** - Gere planilhas Excel com os dados dos equipamentos

Posso explicar qualquer um desses processos em detalhes! üòä"
```

---

## üìä **LOGS PARA VERIFICAR FUNCIONAMENTO**

Ap√≥s fazer uma pergunta, verifique os logs do backend:

```
[TinOne] Processando pergunta: o que posso fazer com equipamentos?
[TinOne] IA habilitada - tentando gerar resposta com Ollama
[TinOne RAG] Buscando contexto relevante
[TinOne RAG] ‚úÖ Contexto montado com 543 caracteres
[Ollama] Gerando resposta para: o que posso fazer com equipamentos?
[Ollama] ‚úÖ Resposta gerada com sucesso
[TinOne] ‚úÖ Resposta gerada com IA
```

Se IA n√£o estiver dispon√≠vel:
```
[TinOne] Ollama n√£o dispon√≠vel, usando resposta gen√©rica
```

---

## ‚ö° **DESEMPENHO E REQUISITOS**

### **Requisitos M√≠nimos:**
- **CPU:** 4 cores
- **RAM:** 8GB (4GB livres)
- **Disco:** 5GB livres
- **Windows:** 10/11 64-bit

### **Recomendado:**
- **CPU:** 8 cores
- **RAM:** 16GB
- **Disco:** SSD com 10GB livres

### **Tempo de Resposta:**
- **Llama 3.2 (3B):** 2-5 segundos
- **Mistral (7B):** 5-10 segundos

---

## üîÑ **TROCAR MODELO**

Para usar um modelo diferente, edite `OllamaService.cs`:

```csharp
public OllamaService(ILogger<OllamaService> logger)
{
    _logger = logger;
    _ollamaUrl = "http://localhost:11434";
    _modelo = "mistral:7b"; // ‚Üê Altere aqui
    // ...
}
```

Recompile e reinicie o backend.

---

## üõ†Ô∏è **TROUBLESHOOTING**

### **Problema: Ollama n√£o inicia**
```bash
# Verificar se porta 11434 est√° ocupada
netstat -ano | findstr :11434

# Matar processo se necess√°rio
taskkill /PID <PID> /F

# Iniciar Ollama
ollama serve
```

### **Problema: Modelo n√£o foi baixado**
```bash
ollama list
ollama pull llama3.2:3b
```

### **Problema: Erro de conex√£o**
Verifique se Ollama est√° rodando:
```bash
curl http://localhost:11434/api/tags
```

### **Problema: Respostas muito lentas**
- Troque para modelo menor (llama3.2:3b)
- Aumente a RAM dispon√≠vel
- Feche outros programas

---

## üéõÔ∏è **AJUSTAR PAR√ÇMETROS DA IA**

Em `OllamaService.cs`, voc√™ pode ajustar:

```csharp
options = new
{
    temperature = 0.7,    // Criatividade (0.0-1.0) - Maior = mais criativo
    top_p = 0.9,          // Diversidade (0.0-1.0)
    max_tokens = 500      // Tamanho m√°ximo da resposta
}
```

---

## üìö **PR√ìXIMOS PASSOS**

1. ‚úÖ **Teste b√°sico** - Pergunte ao Oni coisas simples
2. ‚úÖ **Ajuste fino** - Modifique temperatura e max_tokens
3. ‚úÖ **Expanda FAQ** - Adicione mais conte√∫do em `faq.json`
4. üîÆ **Futuro:** Fine-tuning do modelo com dados espec√≠ficos do SingleOne

---

## üÜò **SUPORTE**

- **Ollama Docs:** https://ollama.com/docs
- **Modelos dispon√≠veis:** https://ollama.com/library
- **GitHub Ollama:** https://github.com/ollama/ollama

---

## ‚úÖ **CHECKLIST COMPLETO**

- [ ] Ollama instalado e funcionando
- [ ] Modelo baixado (llama3.2:3b ou outro)
- [ ] `ollama serve` rodando
- [ ] Backend recompilado e reiniciado
- [ ] IA habilitada nas configura√ß√µes do Oni
- [ ] Teste realizado com sucesso

---

üéâ **Pronto! Seu Oni agora √© inteligente de verdade!** ü¶âü§ñ

